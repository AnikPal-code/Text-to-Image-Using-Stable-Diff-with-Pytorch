{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRO0yeUGWdbD",
        "outputId": "9c99d8b1-3d18-4df3-8eb7-d79873713040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1: Conv2d\n",
            "  Input channels: 3\n",
            "  Output channels: 128\n",
            "  Kernel size: (3, 3)\n",
            "  Stride: (1, 1)\n",
            "  Padding: (1, 1)\n",
            "\n",
            "Layer 2: VAE_ResidualBlock\n",
            "  In channels: 128\n",
            "  Out channels: 128\n",
            "\n",
            "Layer 3: VAE_ResidualBlock\n",
            "  In channels: 128\n",
            "  Out channels: 128\n",
            "\n",
            "Layer 4: Conv2d\n",
            "  Input channels: 128\n",
            "  Output channels: 128\n",
            "  Kernel size: (3, 3)\n",
            "  Stride: (2, 2)\n",
            "  Padding: (0, 0)\n",
            "\n",
            "Layer 5: VAE_ResidualBlock\n",
            "  In channels: 128\n",
            "  Out channels: 256\n",
            "\n",
            "Layer 6: VAE_ResidualBlock\n",
            "  In channels: 256\n",
            "  Out channels: 256\n",
            "\n",
            "Layer 7: Conv2d\n",
            "  Input channels: 256\n",
            "  Output channels: 256\n",
            "  Kernel size: (3, 3)\n",
            "  Stride: (2, 2)\n",
            "  Padding: (0, 0)\n",
            "\n",
            "Layer 8: VAE_ResidualBlock\n",
            "  In channels: 256\n",
            "  Out channels: 512\n",
            "\n",
            "Layer 9: VAE_ResidualBlock\n",
            "  In channels: 512\n",
            "  Out channels: 512\n",
            "\n",
            "Layer 10: Conv2d\n",
            "  Input channels: 512\n",
            "  Output channels: 512\n",
            "  Kernel size: (3, 3)\n",
            "  Stride: (2, 2)\n",
            "  Padding: (0, 0)\n",
            "\n",
            "Layer 11: VAE_ResidualBlock\n",
            "  In channels: 512\n",
            "  Out channels: 512\n",
            "\n",
            "Layer 12: VAE_ResidualBlock\n",
            "  In channels: 512\n",
            "  Out channels: 512\n",
            "\n",
            "Layer 13: VAE_ResidualBlock\n",
            "  In channels: 512\n",
            "  Out channels: 512\n",
            "\n",
            "Layer 14: VAE_AttentionBlock\n",
            "  Channels: 512\n",
            "\n",
            "Layer 15: VAE_ResidualBlock\n",
            "  In channels: 512\n",
            "  Out channels: 512\n",
            "\n",
            "Layer 16: GroupNorm\n",
            "  Number of groups: 32\n",
            "  Number of channels: 512\n",
            "\n",
            "Layer 17: SiLU\n",
            "  Activation function: SiLU (Swish)\n",
            "\n",
            "Layer 18: Conv2d\n",
            "  Input channels: 512\n",
            "  Output channels: 8\n",
            "  Kernel size: (3, 3)\n",
            "  Stride: (1, 1)\n",
            "  Padding: (1, 1)\n",
            "\n",
            "Layer 19: Conv2d\n",
            "  Input channels: 8\n",
            "  Output channels: 8\n",
            "  Kernel size: (1, 1)\n",
            "  Stride: (1, 1)\n",
            "  Padding: (0, 0)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define placeholder classes for VAE_ResidualBlock and VAE_AttentionBlock\n",
        "class VAE_ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "class VAE_AttentionBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "\n",
        "class VAE_Encoder(nn.Sequential):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
        "            VAE_ResidualBlock(128, 128),\n",
        "            VAE_ResidualBlock(128, 128),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=0),\n",
        "            VAE_ResidualBlock(128, 256),\n",
        "            VAE_ResidualBlock(256, 256),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=0),\n",
        "            VAE_ResidualBlock(256, 512),\n",
        "            VAE_ResidualBlock(512, 512),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=0),\n",
        "            VAE_ResidualBlock(512, 512),\n",
        "            VAE_ResidualBlock(512, 512),\n",
        "            VAE_ResidualBlock(512, 512),\n",
        "            VAE_AttentionBlock(512),\n",
        "            VAE_ResidualBlock(512, 512),\n",
        "            nn.GroupNorm(32, 512),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(512, 8, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(8, 8, kernel_size=1, padding=0),\n",
        "        )\n",
        "\n",
        "# Create an instance of VAE_Encoder\n",
        "encoder = VAE_Encoder()\n",
        "\n",
        "# Print the layers\n",
        "for idx, layer in enumerate(encoder):\n",
        "    print(f\"Layer {idx + 1}: {layer.__class__.__name__}\")\n",
        "    if isinstance(layer, nn.Conv2d):\n",
        "        print(f\"  Input channels: {layer.in_channels}\")\n",
        "        print(f\"  Output channels: {layer.out_channels}\")\n",
        "        print(f\"  Kernel size: {layer.kernel_size}\")\n",
        "        print(f\"  Stride: {layer.stride}\")\n",
        "        print(f\"  Padding: {layer.padding}\")\n",
        "    elif isinstance(layer, VAE_ResidualBlock):\n",
        "        print(f\"  In channels: {layer.in_channels}\")\n",
        "        print(f\"  Out channels: {layer.out_channels}\")\n",
        "    elif isinstance(layer, VAE_AttentionBlock):\n",
        "        print(f\"  Channels: {layer.channels}\")\n",
        "    elif isinstance(layer, nn.GroupNorm):\n",
        "        print(f\"  Number of groups: {layer.num_groups}\")\n",
        "        print(f\"  Number of channels: {layer.num_channels}\")\n",
        "    elif isinstance(layer, nn.SiLU):\n",
        "        print(\"  Activation function: SiLU (Swish)\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoU-EqErWeHL",
        "outputId": "354fe418-3cab-460e-9c13-fab098863a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, n_heads, d_embed):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.d_embed = d_embed\n",
        "\n",
        "class VAE_AttentionBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.groupnorm = nn.GroupNorm(32, channels)\n",
        "        self.attention = SelfAttention(1, channels)\n",
        "\n",
        "class VAE_ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.groupnorm_1 = nn.GroupNorm(32, in_channels)\n",
        "        self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.groupnorm_2 = nn.GroupNorm(32, out_channels)\n",
        "        self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        if in_channels == out_channels:\n",
        "            self.residual_layer = nn.Identity()\n",
        "        else:\n",
        "            self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
        "\n",
        "class VAE_Decoder(nn.Sequential):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            nn.Conv2d(4, 4, kernel_size=1, padding=0),\n",
        "            nn.Conv2d(4, 512, kernel_size=3, padding=1),\n",
        "            VAE_ResidualBlock(512, 512),\n",
        "            VAE_AttentionBlock(512),\n",
        "            VAE_ResidualBlock(512, 512),\n",
        "            VAE_ResidualBlock(512, 512),\n",
        "            VAE_ResidualBlock(512, 512),\n",
        "            VAE_ResidualBlock(512, 512),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            VAE_ResidualBlock(512, 512),\n",
        "            VAE_ResidualBlock(512, 512),\n",
        "            VAE_ResidualBlock(512, 512),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            VAE_ResidualBlock(512, 256),\n",
        "            VAE_ResidualBlock(256, 256),\n",
        "            VAE_ResidualBlock(256, 256),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            VAE_ResidualBlock(256, 128),\n",
        "            VAE_ResidualBlock(128, 128),\n",
        "            VAE_ResidualBlock(128, 128),\n",
        "            nn.GroupNorm(32, 128),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(128, 3, kernel_size=3, padding=1),\n",
        "        )\n",
        "\n",
        "# Create an instance of VAE_Decoder\n",
        "decoder = VAE_Decoder()\n",
        "\n",
        "# Print the layers\n",
        "for idx, layer in enumerate(decoder):\n",
        "    print(f\"Layer {idx + 1}: {layer.__class__.__name__}\")\n",
        "    if isinstance(layer, nn.Conv2d):\n",
        "        print(f\"  Input channels: {layer.in_channels}\")\n",
        "        print(f\"  Output channels: {layer.out_channels}\")\n",
        "        print(f\"  Kernel size: {layer.kernel_size}\")\n",
        "        print(f\"  Stride: {layer.stride}\")\n",
        "        print(f\"  Padding: {layer.padding}\")\n",
        "    elif isinstance(layer, VAE_ResidualBlock):\n",
        "        print(f\"  In channels: {layer.in_channels}\")\n",
        "        print(f\"  Out channels: {layer.out_channels}\")\n",
        "    elif isinstance(layer, VAE_AttentionBlock):\n",
        "        print(f\"  Channels: {layer.groupnorm.num_channels}\")\n",
        "    elif isinstance(layer, nn.Upsample):\n",
        "        print(f\"  Scale factor: {layer.scale_factor}\")\n",
        "    elif isinstance(layer, nn.GroupNorm):\n",
        "        print(f\"  Number of groups: {layer.num_groups}\")\n",
        "        print(f\"  Number of channels: {layer.num_channels}\")\n",
        "    elif isinstance(layer, nn.SiLU):\n",
        "        print(\"  Activation function: SiLU (Swish)\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "4e5f0LqPW-EX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8af9b86b-e59f-485b-a9fc-f0dfce8baa72"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1: Conv2d\n",
            "  Input channels: 4\n",
            "  Output channels: 4\n",
            "  Kernel size: (1, 1)\n",
            "  Stride: (1, 1)\n",
            "  Padding: (0, 0)\n",
            "\n",
            "Layer 2: Conv2d\n",
            "  Input channels: 4\n",
            "  Output channels: 512\n",
            "  Kernel size: (3, 3)\n",
            "  Stride: (1, 1)\n",
            "  Padding: (1, 1)\n",
            "\n",
            "Layer 3: VAE_ResidualBlock\n",
            "  In channels: 512\n",
            "  Out channels: 512\n",
            "\n",
            "Layer 4: VAE_AttentionBlock\n",
            "  Channels: 512\n",
            "\n",
            "Layer 5: VAE_ResidualBlock\n",
            "  In channels: 512\n",
            "  Out channels: 512\n",
            "\n",
            "Layer 6: VAE_ResidualBlock\n",
            "  In channels: 512\n",
            "  Out channels: 512\n",
            "\n",
            "Layer 7: VAE_ResidualBlock\n",
            "  In channels: 512\n",
            "  Out channels: 512\n",
            "\n",
            "Layer 8: VAE_ResidualBlock\n",
            "  In channels: 512\n",
            "  Out channels: 512\n",
            "\n",
            "Layer 9: Upsample\n",
            "  Scale factor: 2.0\n",
            "\n",
            "Layer 10: Conv2d\n",
            "  Input channels: 512\n",
            "  Output channels: 512\n",
            "  Kernel size: (3, 3)\n",
            "  Stride: (1, 1)\n",
            "  Padding: (1, 1)\n",
            "\n",
            "Layer 11: VAE_ResidualBlock\n",
            "  In channels: 512\n",
            "  Out channels: 512\n",
            "\n",
            "Layer 12: VAE_ResidualBlock\n",
            "  In channels: 512\n",
            "  Out channels: 512\n",
            "\n",
            "Layer 13: VAE_ResidualBlock\n",
            "  In channels: 512\n",
            "  Out channels: 512\n",
            "\n",
            "Layer 14: Upsample\n",
            "  Scale factor: 2.0\n",
            "\n",
            "Layer 15: Conv2d\n",
            "  Input channels: 512\n",
            "  Output channels: 512\n",
            "  Kernel size: (3, 3)\n",
            "  Stride: (1, 1)\n",
            "  Padding: (1, 1)\n",
            "\n",
            "Layer 16: VAE_ResidualBlock\n",
            "  In channels: 512\n",
            "  Out channels: 256\n",
            "\n",
            "Layer 17: VAE_ResidualBlock\n",
            "  In channels: 256\n",
            "  Out channels: 256\n",
            "\n",
            "Layer 18: VAE_ResidualBlock\n",
            "  In channels: 256\n",
            "  Out channels: 256\n",
            "\n",
            "Layer 19: Upsample\n",
            "  Scale factor: 2.0\n",
            "\n",
            "Layer 20: Conv2d\n",
            "  Input channels: 256\n",
            "  Output channels: 256\n",
            "  Kernel size: (3, 3)\n",
            "  Stride: (1, 1)\n",
            "  Padding: (1, 1)\n",
            "\n",
            "Layer 21: VAE_ResidualBlock\n",
            "  In channels: 256\n",
            "  Out channels: 128\n",
            "\n",
            "Layer 22: VAE_ResidualBlock\n",
            "  In channels: 128\n",
            "  Out channels: 128\n",
            "\n",
            "Layer 23: VAE_ResidualBlock\n",
            "  In channels: 128\n",
            "  Out channels: 128\n",
            "\n",
            "Layer 24: GroupNorm\n",
            "  Number of groups: 32\n",
            "  Number of channels: 128\n",
            "\n",
            "Layer 25: SiLU\n",
            "  Activation function: SiLU (Swish)\n",
            "\n",
            "Layer 26: Conv2d\n",
            "  Input channels: 128\n",
            "  Output channels: 3\n",
            "  Kernel size: (3, 3)\n",
            "  Stride: (1, 1)\n",
            "  Padding: (1, 1)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "prdLztixXEc_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}